{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnotherSamWithADream/RetinalDiseaseDetectionDL/blob/main/vit_detection_of_retinography_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI5KpkfSF44U"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "from itertools import chain\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tQXnPEmOE_O",
        "outputId": "d936fd59-c3c1-4c02-eb1a-0b06d298ccad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLSs3nO6Hg5I"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZoVtb7cF44V",
        "outputId": "9a799d0c-2502-4802-ee44-0ff061f57342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "print(f\"Torch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgOYZoZkN6x5"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFRq9hDjN5NW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Step 2: Define the folder in Google Drive where the datasets will be saved\n",
        "# drive_folder = '/content/drive/MyDrive/KaggleDatasets/'\n",
        "\n",
        "# # Create the destination folder in Google Drive if it doesn't exist\n",
        "# os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# # Step 3: Download the first dataset (resized 2015-2019 blindness detection images)\n",
        "# dataset_path_1 = kagglehub.dataset_download(\"benjaminwarner/resized-2015-2019-blindness-detection-images\")\n",
        "# print(\"First dataset downloaded to:\", dataset_path_1)\n",
        "\n",
        "# # Step 4: Copy the first dataset to Google Drive\n",
        "# drive_path_1 = os.path.join(drive_folder, \"resized-2015-2019-blindness-detection-images/\")\n",
        "# shutil.copytree(dataset_path_1, drive_path_1, dirs_exist_ok=True)\n",
        "# print(f\"First dataset copied to: {drive_path_1}\")\n",
        "\n",
        "# # Step 5: Download the second competition dataset (aptos2019-blindness-detection)\n",
        "# dataset_path_2 = kagglehub.competition_download(\"aptos2019-blindness-detection\")\n",
        "# print(\"Second competition dataset downloaded to:\", dataset_path_2)\n",
        "\n",
        "# # Step 6: Copy the second dataset to Google Drive\n",
        "# drive_path_2 = os.path.join(drive_folder, \"aptos2019-blindness-detection/\")\n",
        "# shutil.copytree(dataset_path_2, drive_path_2, dirs_exist_ok=True)\n",
        "# print(f\"Second dataset copied to: {drive_path_2}\")\n",
        "\n",
        "\n",
        "\n",
        "# dataset_path_3 = kagglehub.dataset_download(\"pineapplepencil/custom-transform-blindness-2019\")\n",
        "# print(\"Third competition dataset downloaded to:\", dataset_path_3)\n",
        "\n",
        "# # Step 6: Copy the second dataset to Google Drive\n",
        "# drive_path_3 = os.path.join(drive_folder, \"custom-transform-blindness-2019/\")\n",
        "# shutil.copytree(dataset_path_2, drive_path_3, dirs_exist_ok=True)\n",
        "# print(f\"Third dataset copied to: {drive_path_3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEvhlvSSF44W"
      },
      "source": [
        "### PARAMETERS SELLECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RkFgxcwjOr6"
      },
      "outputs": [],
      "source": [
        "# # prompt: find count of files by folder in a folder tree\n",
        "\n",
        "# import os\n",
        "\n",
        "# def count_files_by_folder(root_folder):\n",
        "#   \"\"\"Counts the number of files in each subfolder of a given root folder.\n",
        "\n",
        "#   Args:\n",
        "#     root_folder: The path to the root folder.\n",
        "\n",
        "#   Returns:\n",
        "#     A dictionary where keys are subfolder paths and values are the number of files\n",
        "#     in each subfolder.\n",
        "#   \"\"\"\n",
        "#   file_counts = {}\n",
        "#   for dirpath, dirnames, filenames in os.walk(root_folder):\n",
        "#     file_counts[dirpath] = len(filenames)\n",
        "#   return file_counts\n",
        "\n",
        "# # Example usage:\n",
        "# root_folder = '/content/drive/MyDrive/KaggleDatasets/'  # Replace with your root folder\n",
        "# folder_file_counts = count_files_by_folder(root_folder)\n",
        "\n",
        "# for folder, count in folder_file_counts.items():\n",
        "#   print(f\"Folder: {folder}, File Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ugrkm6PF44X"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 5e-4\n",
        "gamma = 0.8\n",
        "seed = 42\n",
        "num_classes = 1\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpieJF_WF44X"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2sBcN4oF44X"
      },
      "source": [
        "### PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mn5zxDCF44X"
      },
      "outputs": [],
      "source": [
        "#The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam\n",
        "import cv2\n",
        "\n",
        "def crop_image1(img,tol=7):\n",
        "    # img is image data\n",
        "    # tol  is tolerance\n",
        "\n",
        "    mask = img>tol\n",
        "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "\n",
        "def crop_image_from_gray(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img>tol\n",
        "\n",
        "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
        "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
        "            return img # return original image\n",
        "        else:\n",
        "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
        "    #         print(img1.shape,img2.shape,img3.shape)\n",
        "            img = np.stack([img1,img2,img3],axis=-1)\n",
        "    #         print(img.shape)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHBCQ_omF44X"
      },
      "source": [
        "### TEST IMAGE TRANSFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y50ooN-jCWa"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Path to the folder containing input images\n",
        "# inPath = '/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test_images'\n",
        "\n",
        "# # Path of the folder that will contain the transformed images\n",
        "# outPath = \"test_images_transformed\"\n",
        "# os.makedirs(outPath, exist_ok=True)\n",
        "\n",
        "# # Loop through the images and apply transformations\n",
        "# for imagePath in tqdm(os.listdir(inPath), desc=\"Processing images\"):\n",
        "#     # imagePath contains name of the image\n",
        "#     inputPath = os.path.join(inPath, imagePath)\n",
        "\n",
        "#     image = cv2.imread(inputPath)\n",
        "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#     image = crop_image_from_gray(image)\n",
        "#     image = cv2.resize(image, (224, 224))\n",
        "#     image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n",
        "\n",
        "#     fullOutPath = os.path.join(outPath, imagePath)\n",
        "#     cv2.imwrite(fullOutPath, image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBiEBGazF44X"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# inPath = '/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test_images'\n",
        "\n",
        "# # path of the folder that will contain the modified image\n",
        "# try:\n",
        "#     os.mkdir(\"/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test_images_transformed\")\n",
        "# except:\n",
        "#     print(\"path already exists\")\n",
        "\n",
        "# outPath =\"/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test_images_transformed\"\n",
        "\n",
        "# for imagePath in tqdm(os.listdir(inPath)):\n",
        "#     # imagePath contains name of the image\n",
        "#     inputPath = os.path.join(inPath, imagePath)\n",
        "\n",
        "#     image = cv2.imread(inputPath)\n",
        "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#     image = crop_image_from_gray(image)\n",
        "#     image = cv2.resize(image, (224, 224))\n",
        "#     image = cv2.addWeighted (image,4, cv2.GaussianBlur( image , (0,0) , 30) ,-4 ,128)\n",
        "\n",
        "#     fullOutPath = os.path.join(outPath, imagePath)\n",
        "#     cv2.imwrite(fullOutPath, image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p4V0EVkF44Y"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed'\n",
        "test_dir = '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/test_images_transformed'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWsoPTanF44Y"
      },
      "outputs": [],
      "source": [
        "train_list = glob.glob(os.path.join(train_dir,'*.*'))\n",
        "test_list = glob.glob(os.path.join(test_dir, '*.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6kSHt0zF44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8184de-5454-40b2-bc5e-f9ff08b7f12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data: 0\n",
            "Test Data: 0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train Data: {len(train_list)}\")\n",
        "print(f\"Test Data: {len(test_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crjioLYbF44Y"
      },
      "source": [
        "### LABELING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBt28ob7F44Y"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/train.csv')\n",
        "df_train_old = pd.read_csv(\"/content/drive/MyDrive/KaggleDatasets/resized-2015-2019-blindness-detection-images/labels/trainLabels15.csv\")\n",
        "df_train_old = df_train_old.rename({\"image\" : \"id_code\", \"level\" : \"diagnosis\"}, axis=1)\n",
        "df_train = pd.concat([df_train,df_train_old]).reset_index(drop=True)\n",
        "\n",
        "labels = df_train['diagnosis'].values\n",
        "label_lookup = df_train.set_index('id_code')\n",
        "\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nSFVuqTF44Y"
      },
      "outputs": [],
      "source": [
        "class_weights = df_train['diagnosis'].value_counts()\n",
        "dfs = [df_train[df_train['diagnosis'] == i].sample(class_weights[4]) for i in range(5)]\n",
        "resampled = pd.concat(dfs, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oCPZtp7F44Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f4e56065-66c7-415a-f62e-c78636489fe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "diagnosis\n",
              "0    1003\n",
              "1    1003\n",
              "2    1003\n",
              "3    1003\n",
              "4    1003\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diagnosis</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "resampled.diagnosis.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqWPnwOMF44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e88b9a-4c9d-44b1-b9ea-5bbb3e9d1700"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed/19473_right.jpg',\n",
              "       '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed/23139_left.jpg',\n",
              "       '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed/4274_right.jpg',\n",
              "       ...,\n",
              "       '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed/4a693dd3921a.png',\n",
              "       '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed/3802_left.jpg',\n",
              "       '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed/61e301bd3c25.png'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "new_train_list = (train_dir + '/' + resampled['id_code'].apply(lambda x: x + ('.jpg' if '_' in x else '.png'))).values\n",
        "new_train_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5uEZxTyF44Y"
      },
      "source": [
        "#### ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBxG9JCkF44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee442970-f87b-4f5e-9fdf-6432684e37a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38788, 5)\n"
          ]
        }
      ],
      "source": [
        "y_train = pd.get_dummies(df_train['diagnosis']).values\n",
        "\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLUm2iRnF44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a95206-5394-4efd-f211-715b5aec975e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original y_train: [27615  2813  6291  1066  1003]\n",
            "Multilabel version: [38788 11173  8360  2069  1003]\n"
          ]
        }
      ],
      "source": [
        "y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\n",
        "y_train_multi[:, 4] = y_train[:, 4]\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n",
        "\n",
        "print(\"Original y_train:\", y_train.sum(axis=0))\n",
        "print(\"Multilabel version:\", y_train_multi.sum(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ixrLypcF44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78cfcdc-f910-4c88-e2d2-970e75f42b68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, False, False],\n",
              "       [ True,  True,  True,  True,  True],\n",
              "       [ True,  True, False, False, False],\n",
              "       ...,\n",
              "       [ True, False, False, False, False],\n",
              "       [ True, False, False, False, False],\n",
              "       [ True,  True, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "y_train_multi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfP7feFeF44Z"
      },
      "source": [
        "### images VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmjcDRcvF44Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50842af-c6c6-4b0c-94d8-6c0794e8c5bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "get_index = lambda x : df_train[df_train.id_code == x].index[0]\n",
        "y_train_multi[get_index('0a4e1a29ffff')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR1TDEg1F44Z"
      },
      "source": [
        "#### examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEMX1IwpF44Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "628bc9d0-d35d-4dce-eee0-67ad06fa2039"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "low >= high",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-199d0a103623>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mnumpy/random/_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: low >= high"
          ]
        }
      ],
      "source": [
        "random_idx = np.random.randint(1, len(train_list), size=9)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "\n",
        "for idx, ax in enumerate(axes.ravel()):\n",
        "    img = Image.open(train_list[idx])\n",
        "    name = train_list[idx].split(\"/\")[-1].split(\".\")[0]\n",
        "    ax.set_title('label = '+ str(labels[idx]) + \", file = \" + name)\n",
        "    ax.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdrOVJc0F44Z"
      },
      "source": [
        "### DATASET SPLITTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK_gPpAuF44Z"
      },
      "outputs": [],
      "source": [
        "train_list, valid_list = train_test_split(new_train_list,\n",
        "                                          test_size=0.05,\n",
        "                                          random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k35NkSIHF44Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271bd83d-d2bd-41c1-8979-4d823670aa77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4764\n",
            "251\n"
          ]
        }
      ],
      "source": [
        "print(len(train_list))\n",
        "print(len(valid_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrjV_k60F44Z"
      },
      "source": [
        "#### AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koz_vs-YF44Z"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "#         transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM7RyCx6F44Z"
      },
      "source": [
        "#### Making the Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4x3n4XEF44Z"
      },
      "outputs": [],
      "source": [
        "class Blindness2019(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        label = label_lookup.loc[img_path.split(\"/\")[-1].split(\".\")[0]][0]\n",
        "#         label = torch.tensor(label).to(torch.float32)\n",
        "        image_id = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "#         label = y_train_multi[get_index(image_id)]\n",
        "#         label = y_train_multi[random.randint(0,3000)]\n",
        "        return img_transformed, label\n",
        "\n",
        "class Blindness2019Test(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        return img_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCjDw_x6F44Z"
      },
      "source": [
        "#### Instantiating the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbZgwYPbF44Z"
      },
      "outputs": [],
      "source": [
        "train_data = Blindness2019(train_list, transform=train_transforms)\n",
        "val_data = Blindness2019(valid_list, transform=test_transforms)\n",
        "test_data = Blindness2019Test(test_list, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20M9jV4eF44a"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset = val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAcPVdquF44a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57706fcd-9033-458c-aa78-7d5e27577e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4764 75\n",
            "251 4\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data), len(train_loader))\n",
        "print(len(val_data), len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2V7MR0kF44a"
      },
      "source": [
        "## Setting Up the Vision Transformer (ViT) Model\n",
        "\n",
        "We'll be using the ViT model pre-trained on a large dataset and fine-tuning it on our chest X-ray dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZncXSuV2F44a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "28ece3f90f724dfc98318bf17a1669a9",
            "e304689b6cb7464386f5b36e26e10e74",
            "7469892936794550b92dcca7c707d1e2",
            "efbdacaf51dd40d2b6059af9f14ae3fa",
            "433de4f1cb164146b6151aaee319c45a",
            "b988851fe753433a8cfe5e1f8731b616",
            "8a6aab63172d4fde87f6e87c3fcae8ea",
            "ae1eb53818d84ca2b1b3a38bdf20d971",
            "54c3944971bb452aae1d4f3ad6e4ce27",
            "6fcba25afd9c486588e91de1f9088520",
            "e7358944608d42ee91b368bf490d6d7a",
            "a32692250f124cf288622c54419d8d09",
            "447a4454bf90487e8f5b84cb19b59377",
            "a9ab2d37e4c0411a9d5a598a2d33be0c",
            "e127fbc98b07448782cf6b932bba838e",
            "6b89bbd8cd8f4b6285bc2d6623748272",
            "42014fcce7004bb5957ffea15813f13e",
            "cec3d1abfee2405e948d5348d7d5aecb",
            "d01c1cc154e44878b771121be1f015c9",
            "5383056860b64de08b0c7dd4655440ca",
            "0d0be8f047ed468eb21fe501afb39811",
            "5e4fec55bab44fbdb66fc44dace9b630"
          ]
        },
        "outputId": "69260801-6956-4b6f-dd6d-06e3997b39c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28ece3f90f724dfc98318bf17a1669a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a32692250f124cf288622c54419d8d09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTForImageClassification, ViTConfig, ViTImageProcessor\n",
        "\n",
        "# Define the ViT configuration\n",
        "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "config.num_labels = 5  # (0,1,2,3,4)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", config=config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_iAGwdiF44i"
      },
      "source": [
        "## Define Loss and Optimizer\n",
        "\n",
        "We'll use the CrossEntropy loss as it's suitable for binary classification tasks. For optimization, we'll use the Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvpVGxNtF44i"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkzoYKBzF44i"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Let's train the ViT model on our chest X-ray dataset. We'll also validate the model on the validation set after each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK1mR3gpF44i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Check for GPU availability and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    train_losses = []  # List to store training loss for each epoch\n",
        "    val_accuracies = []  # List to store validation accuracy for each epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images).logits  # Get logits from model outputs\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images).logits\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        # Append the computed values to their respective lists\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzywXhdkF44i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f36d3f3-2fcc-4d9b-b1d7-215c21e6b7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-74db7ba29986>:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  label = label_lookup.loc[img_path.split(\"/\")[-1].split(\".\")[0]][0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.4861, Validation Accuracy: 43.43%\n",
            "Epoch 2/10, Loss: 1.2826, Validation Accuracy: 48.61%\n",
            "Epoch 3/10, Loss: 1.1613, Validation Accuracy: 51.39%\n",
            "Epoch 4/10, Loss: 1.0754, Validation Accuracy: 54.58%\n",
            "Epoch 5/10, Loss: 1.0020, Validation Accuracy: 54.18%\n",
            "Epoch 6/10, Loss: 0.9406, Validation Accuracy: 52.99%\n",
            "Epoch 7/10, Loss: 0.8632, Validation Accuracy: 56.57%\n",
            "Epoch 8/10, Loss: 0.7979, Validation Accuracy: 54.18%\n",
            "Epoch 9/10, Loss: 0.7149, Validation Accuracy: 54.98%\n",
            "Epoch 10/10, Loss: 0.6292, Validation Accuracy: 56.18%\n"
          ]
        }
      ],
      "source": [
        "trained_model, train_losses, val_accuracies = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKph0SOQF44i"
      },
      "outputs": [],
      "source": [
        "# Save the model weights\n",
        "torch.save(trained_model.state_dict(), \"vit_detection_of_retinology.pth\")\n",
        "torch.save(trained_model.state_dict(), \"/content/drive/MyDrive/KaggleDatasets/vit_detection_of_retinology.pth\")\n",
        "\n",
        "# to load the model in the future\n",
        "# model.load_state_dict(torch.load(\"vit_chest_xray_model.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQjZqU2QF44i"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Let's evaluate the trained ViT model on the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ujfMtPmF44i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "8c52b0bc-af46-4c1f-ba20-0334de31ac1e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0707cdc038a1>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz5ACeOkF44i"
      },
      "source": [
        "## Save and Load the Model\n",
        "\n",
        "After training, it's essential to save the model weights to avoid retraining in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60wycacgF44i"
      },
      "source": [
        "## Visualization of Sample Data\n",
        "\n",
        "Displaying a few images from both NORMAL and PNEUMONIA classes to get a feel for the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncQezj91F44i"
      },
      "outputs": [],
      "source": [
        "display_images(train_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDE6BoH4F44i"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        # Training Phase\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images).logits\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFCEAi6cF44i"
      },
      "source": [
        "## Training and Validation Metrics Visualization\n",
        "\n",
        "Plotting the training loss and validation accuracy to understand the model's learning progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYRKPRDqF44j"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plotting training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5sOSaydF44j"
      },
      "source": [
        "From the above graphs, we observe that the training loss decreased over time, which is a positive sign. The validation accuracy remains high, suggesting that the model generalizes well. The dip in accuracy around the second epoch followed by consistent high accuracy indicates that the model might have overcome some initial adaptation challenges but then consistently performed well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydK133oOF44j"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "Visualizing the model's predictions using a confusion matrix to understand its performance in more detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0teNktOvF44j"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes,\n",
        "                yticklabels=classes)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Getting the true labels and the predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        _, predicted = outputs.max(1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Now, we'll plot the confusion matrix\n",
        "labels_list = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
        "plot_confusion_matrix(y_true, y_pred, labels_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7JxnTIEF44j"
      },
      "source": [
        "In the above image, the confusion matrix shows that the model correctly classified 131 patients as having pneumonia (TP) and 389 patients as not having pneumonia (TN). The model incorrectly classified 1 patient as having pneumonia (FP) and 103 patients as not having pneumonia (FN).\n",
        "\n",
        "The overall accuracy of the model is 93.75%, which is good.\n",
        "The accuracy of the model is calculated by dividing the number of true positives and true negatives by the total number of patients. In this case, the accuracy is 93.75%, which means that the model correctly classified 93.75% of the patients.\n",
        "\n",
        "The sensitivity of the model is calculated by dividing the number of true positives by the total number of patients who actually had pneumonia. In this case, the sensitivity is 91.30%, which means that the model correctly identified 91.30% of the patients who actually had pneumonia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_et72csYF44j"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "### Overview:\n",
        "In the realm of medical imaging, the adaptation of the Vision Transformer (ViT) for chest X-ray classification showcases the vast potential of transfer learning. Drawing from its roots in natural language processing, ViT, through self-supervised learning, has successfully ventured into the domain of computer vision, offering a promising solution to detect pneumonia from X-rays.\n",
        "\n",
        "### Key Achievements:\n",
        "1. **ViT's Versatility**: The Vision Transformer's unique approach of segmenting images into patches for processing underscores its versatility. Its core design, which was originally intended for NLP tasks, has been seamlessly repurposed for intricate computer vision challenges.\n",
        "  \n",
        "2. **Impressive Training Dynamics**: Throughout the training phase, a consistent decline in the training loss was observed. This highlights the model's effective learning from the data, optimizing its parameters to reduce inaccuracies.\n",
        "  \n",
        "3. **Stellar Performance Metrics**: The model didn't just stop at learning; it showcased an exemplary generalization capability. The achieved test accuracy of 83.33% stands as a testament to the model's prowess.\n",
        "  \n",
        "4. **Confusion Matrix Insights**: A deep dive into the confusion matrix revealed the model's acute ability to differentiate between 'NORMAL' and 'PNEUMONIA' chest X-rays. Misclassifications were minimal, further solidifying trust in the model's predictions.\n",
        "\n",
        "5. **Self-Supervised Excellence**: One of the crown jewels of the Vision Transformer is its proficiency in self-supervised learning. This feature enables the model to harness vast datasets without explicit labels, inherently generating supervisory signals from the data. This form of learning lays the foundation for its exceptional feature extraction capabilities.\n",
        "\n",
        "### Looking Ahead:\n",
        "The success story of the Vision Transformer in the chest X-ray classification task is a beacon of optimism. Its high accuracy, adaptability, and self-supervised learning capabilities paint a positive picture for its broader applications in the medical imaging domain. As we move forward, it's exciting to think about the myriad of challenges ViT could address, revolutionizing healthcare diagnostics."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 875431,
          "sourceId": 14774,
          "sourceType": "competition"
        },
        {
          "datasetId": 253160,
          "sourceId": 532013,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 251095,
          "sourceId": 848739,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1774742,
          "sourceId": 2896320,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1774759,
          "sourceId": 2896347,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1775253,
          "sourceId": 2900791,
          "sourceType": "datasetVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 1376,
          "modelInstanceId": 3433,
          "sourceId": 4643,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30145,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28ece3f90f724dfc98318bf17a1669a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e304689b6cb7464386f5b36e26e10e74",
              "IPY_MODEL_7469892936794550b92dcca7c707d1e2",
              "IPY_MODEL_efbdacaf51dd40d2b6059af9f14ae3fa"
            ],
            "layout": "IPY_MODEL_433de4f1cb164146b6151aaee319c45a"
          }
        },
        "e304689b6cb7464386f5b36e26e10e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b988851fe753433a8cfe5e1f8731b616",
            "placeholder": "​",
            "style": "IPY_MODEL_8a6aab63172d4fde87f6e87c3fcae8ea",
            "value": "config.json: 100%"
          }
        },
        "7469892936794550b92dcca7c707d1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1eb53818d84ca2b1b3a38bdf20d971",
            "max": 502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54c3944971bb452aae1d4f3ad6e4ce27",
            "value": 502
          }
        },
        "efbdacaf51dd40d2b6059af9f14ae3fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fcba25afd9c486588e91de1f9088520",
            "placeholder": "​",
            "style": "IPY_MODEL_e7358944608d42ee91b368bf490d6d7a",
            "value": " 502/502 [00:00&lt;00:00, 13.7kB/s]"
          }
        },
        "433de4f1cb164146b6151aaee319c45a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b988851fe753433a8cfe5e1f8731b616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6aab63172d4fde87f6e87c3fcae8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae1eb53818d84ca2b1b3a38bdf20d971": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c3944971bb452aae1d4f3ad6e4ce27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fcba25afd9c486588e91de1f9088520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7358944608d42ee91b368bf490d6d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a32692250f124cf288622c54419d8d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_447a4454bf90487e8f5b84cb19b59377",
              "IPY_MODEL_a9ab2d37e4c0411a9d5a598a2d33be0c",
              "IPY_MODEL_e127fbc98b07448782cf6b932bba838e"
            ],
            "layout": "IPY_MODEL_6b89bbd8cd8f4b6285bc2d6623748272"
          }
        },
        "447a4454bf90487e8f5b84cb19b59377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42014fcce7004bb5957ffea15813f13e",
            "placeholder": "​",
            "style": "IPY_MODEL_cec3d1abfee2405e948d5348d7d5aecb",
            "value": "model.safetensors: 100%"
          }
        },
        "a9ab2d37e4c0411a9d5a598a2d33be0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d01c1cc154e44878b771121be1f015c9",
            "max": 345579424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5383056860b64de08b0c7dd4655440ca",
            "value": 345579424
          }
        },
        "e127fbc98b07448782cf6b932bba838e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0be8f047ed468eb21fe501afb39811",
            "placeholder": "​",
            "style": "IPY_MODEL_5e4fec55bab44fbdb66fc44dace9b630",
            "value": " 346M/346M [00:01&lt;00:00, 245MB/s]"
          }
        },
        "6b89bbd8cd8f4b6285bc2d6623748272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42014fcce7004bb5957ffea15813f13e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec3d1abfee2405e948d5348d7d5aecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d01c1cc154e44878b771121be1f015c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5383056860b64de08b0c7dd4655440ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d0be8f047ed468eb21fe501afb39811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e4fec55bab44fbdb66fc44dace9b630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}