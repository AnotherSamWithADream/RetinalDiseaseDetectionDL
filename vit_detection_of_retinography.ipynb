{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 14774,
          "databundleVersionId": 875431,
          "sourceType": "competition"
        },
        {
          "sourceId": 532013,
          "sourceType": "datasetVersion",
          "datasetId": 253160
        },
        {
          "sourceId": 848739,
          "sourceType": "datasetVersion",
          "datasetId": 251095
        },
        {
          "sourceId": 2896320,
          "sourceType": "datasetVersion",
          "datasetId": 1774742
        },
        {
          "sourceId": 2896347,
          "sourceType": "datasetVersion",
          "datasetId": 1774759
        },
        {
          "sourceId": 2900791,
          "sourceType": "datasetVersion",
          "datasetId": 1775253
        },
        {
          "sourceId": 4643,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 3433,
          "modelId": 1376
        }
      ],
      "dockerImageVersionId": 30145,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "from itertools import chain\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:23:01.938768Z",
          "iopub.execute_input": "2024-10-18T00:23:01.939104Z",
          "iopub.status.idle": "2024-10-18T00:23:01.951265Z",
          "shell.execute_reply.started": "2024-10-18T00:23:01.939060Z",
          "shell.execute_reply": "2024-10-18T00:23:01.950121Z"
        },
        "trusted": true,
        "id": "dI5KpkfSF44U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6tQXnPEmOE_O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "b4825c6e-0518-4ed5-c018-bc5b62f9a193"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3b8a479202a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "RLSs3nO6Hg5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Torch: {torch.__version__}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:23:01.931490Z",
          "iopub.execute_input": "2024-10-18T00:23:01.931756Z",
          "iopub.status.idle": "2024-10-18T00:23:01.937264Z",
          "shell.execute_reply.started": "2024-10-18T00:23:01.931723Z",
          "shell.execute_reply": "2024-10-18T00:23:01.936227Z"
        },
        "trusted": true,
        "id": "zZoVtb7cF44V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "FgOYZoZkN6x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Define the folder in Google Drive where the datasets will be saved\n",
        "drive_folder = '/content/drive/MyDrive/KaggleDatasets/'\n",
        "\n",
        "# Create the destination folder in Google Drive if it doesn't exist\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# Step 3: Download the first dataset (resized 2015-2019 blindness detection images)\n",
        "dataset_path_1 = kagglehub.dataset_download(\"benjaminwarner/resized-2015-2019-blindness-detection-images\")\n",
        "print(\"First dataset downloaded to:\", dataset_path_1)\n",
        "\n",
        "# Step 4: Copy the first dataset to Google Drive\n",
        "drive_path_1 = os.path.join(drive_folder, \"resized-2015-2019-blindness-detection-images/\")\n",
        "shutil.copytree(dataset_path_1, drive_path_1, dirs_exist_ok=True)\n",
        "print(f\"First dataset copied to: {drive_path_1}\")\n",
        "\n",
        "# Step 5: Download the second competition dataset (aptos2019-blindness-detection)\n",
        "dataset_path_2 = kagglehub.competition_download(\"aptos2019-blindness-detection\")\n",
        "print(\"Second competition dataset downloaded to:\", dataset_path_2)\n",
        "\n",
        "# Step 6: Copy the second dataset to Google Drive\n",
        "drive_path_2 = os.path.join(drive_folder, \"aptos2019-blindness-detection/\")\n",
        "shutil.copytree(dataset_path_2, drive_path_2, dirs_exist_ok=True)\n",
        "print(f\"Second dataset copied to: {drive_path_2}\")\n",
        "\n",
        "\n",
        "\n",
        "dataset_path_3 = kagglehub.dataset_download(\"pineapplepencil/custom-transform-blindness-2019\")\n",
        "print(\"Third competition dataset downloaded to:\", dataset_path_3)\n",
        "\n",
        "# Step 6: Copy the second dataset to Google Drive\n",
        "drive_path_3 = os.path.join(drive_folder, \"custom-transform-blindness-2019/\")\n",
        "shutil.copytree(dataset_path_2, drive_path_3, dirs_exist_ok=True)\n",
        "print(f\"Third dataset copied to: {drive_path_3}\")"
      ],
      "metadata": {
        "id": "kFRq9hDjN5NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PARAMETERS SELLECTION"
      ],
      "metadata": {
        "id": "fEvhlvSSF44W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yc9qjFWrxha6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 5e-4\n",
        "gamma = 0.8\n",
        "seed = 42\n",
        "num_classes = 1\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:24:11.023657Z",
          "iopub.execute_input": "2024-10-18T00:24:11.023962Z",
          "iopub.status.idle": "2024-10-18T00:24:11.028849Z",
          "shell.execute_reply.started": "2024-10-18T00:24:11.023930Z",
          "shell.execute_reply": "2024-10-18T00:24:11.027950Z"
        },
        "trusted": true,
        "id": "1ugrkm6PF44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:24:15.759661Z",
          "iopub.execute_input": "2024-10-18T00:24:15.759989Z",
          "iopub.status.idle": "2024-10-18T00:24:15.769660Z",
          "shell.execute_reply.started": "2024-10-18T00:24:15.759950Z",
          "shell.execute_reply": "2024-10-18T00:24:15.768829Z"
        },
        "trusted": true,
        "id": "YpieJF_WF44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPROCESSING"
      ],
      "metadata": {
        "id": "D2sBcN4oF44X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam\n",
        "import cv2\n",
        "\n",
        "def crop_image1(img,tol=7):\n",
        "    # img is image data\n",
        "    # tol  is tolerance\n",
        "\n",
        "    mask = img>tol\n",
        "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "\n",
        "def crop_image_from_gray(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img>tol\n",
        "\n",
        "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
        "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
        "            return img # return original image\n",
        "        else:\n",
        "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
        "    #         print(img1.shape,img2.shape,img3.shape)\n",
        "            img = np.stack([img1,img2,img3],axis=-1)\n",
        "    #         print(img.shape)\n",
        "        return img\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:24:19.729261Z",
          "iopub.execute_input": "2024-10-18T00:24:19.729621Z",
          "iopub.status.idle": "2024-10-18T00:24:19.939252Z",
          "shell.execute_reply.started": "2024-10-18T00:24:19.729575Z",
          "shell.execute_reply": "2024-10-18T00:24:19.938343Z"
        },
        "trusted": true,
        "id": "5mn5zxDCF44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST IMAGE TRANSFORMATION"
      ],
      "metadata": {
        "id": "xHBCQ_omF44X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Path to the folder containing input images\n",
        "inPath = '/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test_images'\n",
        "\n",
        "# Path of the folder that will contain the transformed images\n",
        "outPath = \"test_images_transformed\"\n",
        "os.makedirs(outPath, exist_ok=True)\n",
        "\n",
        "# Loop through the images and apply transformations\n",
        "for imagePath in tqdm(os.listdir(inPath), desc=\"Processing images\"):\n",
        "    # imagePath contains name of the image\n",
        "    inputPath = os.path.join(inPath, imagePath)\n",
        "\n",
        "    image = cv2.imread(inputPath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = crop_image_from_gray(image)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n",
        "\n",
        "    fullOutPath = os.path.join(outPath, imagePath)\n",
        "    cv2.imwrite(fullOutPath, image)\n"
      ],
      "metadata": {
        "id": "_Y50ooN-jCWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inPath = '/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test_images'\n",
        "\n",
        "# path of the folder that will contain the modified image\n",
        "try:\n",
        "    os.mkdir(\"test_images_transformed\")\n",
        "except:\n",
        "    print(\"path already exists\")\n",
        "\n",
        "outPath =\"test_images_transformed\"\n",
        "\n",
        "for imagePath in tqdm(os.listdir(inPath)):\n",
        "    # imagePath contains name of the image\n",
        "    inputPath = os.path.join(inPath, imagePath)\n",
        "\n",
        "    image = cv2.imread(inputPath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = crop_image_from_gray(image)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = cv2.addWeighted (image,4, cv2.GaussianBlur( image , (0,0) , 30) ,-4 ,128)\n",
        "\n",
        "    fullOutPath = os.path.join(outPath, imagePath)\n",
        "    cv2.imwrite(fullOutPath, image)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:24:31.260990Z",
          "iopub.execute_input": "2024-10-18T00:24:31.261708Z",
          "iopub.status.idle": "2024-10-18T00:30:27.271837Z",
          "shell.execute_reply.started": "2024-10-18T00:24:31.261666Z",
          "shell.execute_reply": "2024-10-18T00:30:27.271006Z"
        },
        "trusted": true,
        "id": "xBiEBGazF44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/drive/MyDrive/KaggleDatasets/custom-transform-blindness-2019/train_images_transformed'\n",
        "test_dir = './test_images_transformed'\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:30:42.458229Z",
          "iopub.execute_input": "2024-10-18T00:30:42.458538Z",
          "iopub.status.idle": "2024-10-18T00:30:42.462779Z",
          "shell.execute_reply.started": "2024-10-18T00:30:42.458500Z",
          "shell.execute_reply": "2024-10-18T00:30:42.461945Z"
        },
        "trusted": true,
        "id": "9p4V0EVkF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = glob.glob(os.path.join(train_dir,'*.*'))\n",
        "test_list = glob.glob(os.path.join(test_dir, '*.png'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:30:45.174023Z",
          "iopub.execute_input": "2024-10-18T00:30:45.174355Z",
          "iopub.status.idle": "2024-10-18T00:30:45.777501Z",
          "shell.execute_reply.started": "2024-10-18T00:30:45.174319Z",
          "shell.execute_reply": "2024-10-18T00:30:45.776564Z"
        },
        "trusted": true,
        "id": "gWsoPTanF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Data: {len(train_list)}\")\n",
        "print(f\"Test Data: {len(test_list)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:30:48.790296Z",
          "iopub.execute_input": "2024-10-18T00:30:48.790586Z",
          "iopub.status.idle": "2024-10-18T00:30:48.795719Z",
          "shell.execute_reply.started": "2024-10-18T00:30:48.790556Z",
          "shell.execute_reply": "2024-10-18T00:30:48.794643Z"
        },
        "trusted": true,
        "id": "K6kSHt0zF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LABELING"
      ],
      "metadata": {
        "id": "crjioLYbF44Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/train.csv')\n",
        "df_train_old = pd.read_csv(\"/content/drive/MyDrive/KaggleDatasets/resized-2015-2019-blindness-detection-images/labels/trainLabels15.csv\")\n",
        "df_train_old = df_train_old.rename({\"image\" : \"id_code\", \"level\" : \"diagnosis\"}, axis=1)\n",
        "df_train = df_train.append(df_train_old).reset_index(drop=True)\n",
        "\n",
        "labels = df_train['diagnosis'].values\n",
        "label_lookup = df_train.set_index('id_code')\n",
        "\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/KaggleDatasets/aptos2019-blindness-detection/test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:32:53.519562Z",
          "iopub.execute_input": "2024-10-18T00:32:53.519861Z",
          "iopub.status.idle": "2024-10-18T00:32:53.560840Z",
          "shell.execute_reply.started": "2024-10-18T00:32:53.519830Z",
          "shell.execute_reply": "2024-10-18T00:32:53.559925Z"
        },
        "trusted": true,
        "id": "KBt28ob7F44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = df_train['diagnosis'].value_counts()\n",
        "dfs = [df_train[df_train['diagnosis'] == i].sample(class_weights[4]) for i in range(5)]\n",
        "resampled = pd.concat(dfs, axis = 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:33:54.481629Z",
          "iopub.execute_input": "2024-10-18T00:33:54.481926Z",
          "iopub.status.idle": "2024-10-18T00:33:54.498605Z",
          "shell.execute_reply.started": "2024-10-18T00:33:54.481894Z",
          "shell.execute_reply": "2024-10-18T00:33:54.497616Z"
        },
        "trusted": true,
        "id": "9nSFVuqTF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled.diagnosis.value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:36:47.816686Z",
          "iopub.execute_input": "2024-10-18T00:36:47.817006Z",
          "iopub.status.idle": "2024-10-18T00:36:47.825979Z",
          "shell.execute_reply.started": "2024-10-18T00:36:47.816972Z",
          "shell.execute_reply": "2024-10-18T00:36:47.825092Z"
        },
        "trusted": true,
        "id": "7oCPZtp7F44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_list = (train_dir + '/' + resampled['id_code'].apply(lambda x: x + ('.jpg' if '_' in x else '.png'))).values\n",
        "new_train_list"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:36:50.420142Z",
          "iopub.execute_input": "2024-10-18T00:36:50.420490Z",
          "iopub.status.idle": "2024-10-18T00:36:50.432043Z",
          "shell.execute_reply.started": "2024-10-18T00:36:50.420446Z",
          "shell.execute_reply": "2024-10-18T00:36:50.430971Z"
        },
        "trusted": true,
        "id": "vqWPnwOMF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ENCODING"
      ],
      "metadata": {
        "id": "-5uEZxTyF44Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.get_dummies(df_train['diagnosis']).values\n",
        "\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:02.374960Z",
          "iopub.execute_input": "2024-10-18T00:37:02.375302Z",
          "iopub.status.idle": "2024-10-18T00:37:02.383881Z",
          "shell.execute_reply.started": "2024-10-18T00:37:02.375264Z",
          "shell.execute_reply": "2024-10-18T00:37:02.382724Z"
        },
        "trusted": true,
        "id": "WBxG9JCkF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\n",
        "y_train_multi[:, 4] = y_train[:, 4]\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n",
        "\n",
        "print(\"Original y_train:\", y_train.sum(axis=0))\n",
        "print(\"Multilabel version:\", y_train_multi.sum(axis=0))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:07.112468Z",
          "iopub.execute_input": "2024-10-18T00:37:07.112771Z",
          "iopub.status.idle": "2024-10-18T00:37:07.124094Z",
          "shell.execute_reply.started": "2024-10-18T00:37:07.112737Z",
          "shell.execute_reply": "2024-10-18T00:37:07.123147Z"
        },
        "trusted": true,
        "id": "lLUm2iRnF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_multi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:19.625334Z",
          "iopub.execute_input": "2024-10-18T00:37:19.626181Z",
          "iopub.status.idle": "2024-10-18T00:37:19.633182Z",
          "shell.execute_reply.started": "2024-10-18T00:37:19.626130Z",
          "shell.execute_reply": "2024-10-18T00:37:19.632108Z"
        },
        "trusted": true,
        "id": "7ixrLypcF44Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### images VISUALIZATION"
      ],
      "metadata": {
        "id": "YfP7feFeF44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_index = lambda x : df_train[df_train.id_code == x].index[0]\n",
        "y_train_multi[get_index('0a4e1a29ffff')]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:23.953164Z",
          "iopub.execute_input": "2024-10-18T00:37:23.953471Z",
          "iopub.status.idle": "2024-10-18T00:37:23.965114Z",
          "shell.execute_reply.started": "2024-10-18T00:37:23.953436Z",
          "shell.execute_reply": "2024-10-18T00:37:23.964090Z"
        },
        "trusted": true,
        "id": "fmjcDRcvF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### examples!"
      ],
      "metadata": {
        "id": "CR1TDEg1F44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx = np.random.randint(1, len(train_list), size=9)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "\n",
        "for idx, ax in enumerate(axes.ravel()):\n",
        "    img = Image.open(train_list[idx])\n",
        "    name = train_list[idx].split(\"/\")[-1].split(\".\")[0]\n",
        "    ax.set_title('label = '+ str(labels[idx]) + \", file = \" + name)\n",
        "    ax.imshow(img)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:31.260394Z",
          "iopub.execute_input": "2024-10-18T00:37:31.260683Z",
          "iopub.status.idle": "2024-10-18T00:37:33.146584Z",
          "shell.execute_reply.started": "2024-10-18T00:37:31.260653Z",
          "shell.execute_reply": "2024-10-18T00:37:33.145438Z"
        },
        "trusted": true,
        "id": "IEMX1IwpF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASET SPLITTING"
      ],
      "metadata": {
        "id": "YdrOVJc0F44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_list, valid_list = train_test_split(new_train_list,\n",
        "                                          test_size=0.05,\n",
        "                                          random_state=seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:42.934180Z",
          "iopub.execute_input": "2024-10-18T00:37:42.934503Z",
          "iopub.status.idle": "2024-10-18T00:37:42.942324Z",
          "shell.execute_reply.started": "2024-10-18T00:37:42.934464Z",
          "shell.execute_reply": "2024-10-18T00:37:42.941515Z"
        },
        "trusted": true,
        "id": "qK_gPpAuF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_list))\n",
        "print(len(valid_list))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:45.808615Z",
          "iopub.execute_input": "2024-10-18T00:37:45.808924Z",
          "iopub.status.idle": "2024-10-18T00:37:45.814320Z",
          "shell.execute_reply.started": "2024-10-18T00:37:45.808887Z",
          "shell.execute_reply": "2024-10-18T00:37:45.813397Z"
        },
        "trusted": true,
        "id": "k35NkSIHF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AUGMENTATION"
      ],
      "metadata": {
        "id": "yrjV_k60F44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "#         transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:50.098779Z",
          "iopub.execute_input": "2024-10-18T00:37:50.099542Z",
          "iopub.status.idle": "2024-10-18T00:37:50.105849Z",
          "shell.execute_reply.started": "2024-10-18T00:37:50.099495Z",
          "shell.execute_reply": "2024-10-18T00:37:50.104906Z"
        },
        "trusted": true,
        "id": "Koz_vs-YF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making the Dataset Class"
      ],
      "metadata": {
        "id": "DM7RyCx6F44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Blindness2019(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        label = label_lookup.loc[img_path.split(\"/\")[-1].split(\".\")[0]][0]\n",
        "#         label = torch.tensor(label).to(torch.float32)\n",
        "        image_id = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "#         label = y_train_multi[get_index(image_id)]\n",
        "#         label = y_train_multi[random.randint(0,3000)]\n",
        "        return img_transformed, label\n",
        "\n",
        "class Blindness2019Test(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        return img_transformed"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:37:54.365703Z",
          "iopub.execute_input": "2024-10-18T00:37:54.366027Z",
          "iopub.status.idle": "2024-10-18T00:37:54.376403Z",
          "shell.execute_reply.started": "2024-10-18T00:37:54.365991Z",
          "shell.execute_reply": "2024-10-18T00:37:54.375365Z"
        },
        "trusted": true,
        "id": "J4x3n4XEF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instantiating the dataset class"
      ],
      "metadata": {
        "id": "oCjDw_x6F44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Blindness2019(train_list, transform=train_transforms)\n",
        "valid_data = Blindness2019(valid_list, transform=test_transforms)\n",
        "test_data = Blindness2019Test(test_list, transform=test_transforms)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:38:16.261972Z",
          "iopub.execute_input": "2024-10-18T00:38:16.262324Z",
          "iopub.status.idle": "2024-10-18T00:38:16.267165Z",
          "shell.execute_reply.started": "2024-10-18T00:38:16.262288Z",
          "shell.execute_reply": "2024-10-18T00:38:16.266203Z"
        },
        "trusted": true,
        "id": "NbZgwYPbF44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:38:19.135737Z",
          "iopub.execute_input": "2024-10-18T00:38:19.136036Z",
          "iopub.status.idle": "2024-10-18T00:38:19.141962Z",
          "shell.execute_reply.started": "2024-10-18T00:38:19.136004Z",
          "shell.execute_reply": "2024-10-18T00:38:19.140979Z"
        },
        "trusted": true,
        "id": "20M9jV4eF44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(train_loader))\n",
        "print(len(valid_data), len(valid_loader))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:38:21.668031Z",
          "iopub.execute_input": "2024-10-18T00:38:21.668952Z",
          "iopub.status.idle": "2024-10-18T00:38:21.674485Z",
          "shell.execute_reply.started": "2024-10-18T00:38:21.668906Z",
          "shell.execute_reply": "2024-10-18T00:38:21.673530Z"
        },
        "trusted": true,
        "id": "pAcPVdquF44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers --upgrade"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:43:02.082313Z",
          "iopub.execute_input": "2024-10-18T00:43:02.083160Z",
          "iopub.status.idle": "2024-10-18T00:43:48.793622Z",
          "shell.execute_reply.started": "2024-10-18T00:43:02.083113Z",
          "shell.execute_reply": "2024-10-18T00:43:48.792467Z"
        },
        "trusted": true,
        "id": "8MgW-mfdF44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Vision Transformer (ViT) Model\n",
        "\n",
        "We'll be using the ViT model pre-trained on a large dataset and fine-tuning it on our chest X-ray dataset.\n"
      ],
      "metadata": {
        "id": "S2V7MR0kF44a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTConfig, ViTImageProcessor\n",
        "\n",
        "# Define the ViT configuration\n",
        "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "config.num_labels = 2  # Normal and Pneumonia\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", config=config)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-18T00:38:33.481653Z",
          "iopub.execute_input": "2024-10-18T00:38:33.482001Z",
          "iopub.status.idle": "2024-10-18T00:38:33.834841Z",
          "shell.execute_reply.started": "2024-10-18T00:38:33.481956Z",
          "shell.execute_reply": "2024-10-18T00:38:33.833541Z"
        },
        "trusted": true,
        "id": "ZncXSuV2F44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Loss and Optimizer\n",
        "\n",
        "We'll use the CrossEntropy loss as it's suitable for binary classification tasks. For optimization, we'll use the Adam optimizer.\n"
      ],
      "metadata": {
        "id": "L_iAGwdiF44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "RvpVGxNtF44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Let's train the ViT model on our chest X-ray dataset. We'll also validate the model on the validation set after each epoch.\n"
      ],
      "metadata": {
        "id": "tkzoYKBzF44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Check for GPU availability and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    train_losses = []  # List to store training loss for each epoch\n",
        "    val_accuracies = []  # List to store validation accuracy for each epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images).logits  # Get logits from model outputs\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images).logits\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        # Append the computed values to their respective lists\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n"
      ],
      "metadata": {
        "id": "uK1mR3gpF44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, train_losses, val_accuracies = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "VzywXhdkF44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Let's evaluate the trained ViT model on the test dataset.\n"
      ],
      "metadata": {
        "id": "OQjZqU2QF44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "id": "7ujfMtPmF44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load the Model\n",
        "\n",
        "After training, it's essential to save the model weights to avoid retraining in the future.\n"
      ],
      "metadata": {
        "id": "cz5ACeOkF44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "torch.save(trained_model.state_dict(), \"vit_chest_xray_model.pth\")\n",
        "\n",
        "# to load the model in the future\n",
        "# model.load_state_dict(torch.load(\"vit_chest_xray_model.pth\"))\n"
      ],
      "metadata": {
        "id": "oKph0SOQF44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Sample Data\n",
        "\n",
        "Displaying a few images from both NORMAL and PNEUMONIA classes to get a feel for the data.\n"
      ],
      "metadata": {
        "id": "60wycacgF44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(train_dir)\n"
      ],
      "metadata": {
        "id": "ncQezj91F44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        # Training Phase\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images).logits\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n"
      ],
      "metadata": {
        "id": "vDE6BoH4F44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Metrics Visualization\n",
        "\n",
        "Plotting the training loss and validation accuracy to understand the model's learning progress.\n"
      ],
      "metadata": {
        "id": "lFCEAi6cF44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plotting training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aYRKPRDqF44j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graphs, we observe that the training loss decreased over time, which is a positive sign. The validation accuracy remains high, suggesting that the model generalizes well. The dip in accuracy around the second epoch followed by consistent high accuracy indicates that the model might have overcome some initial adaptation challenges but then consistently performed well."
      ],
      "metadata": {
        "id": "b5sOSaydF44j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "Visualizing the model's predictions using a confusion matrix to understand its performance in more detail.\n"
      ],
      "metadata": {
        "id": "ydK133oOF44j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes,\n",
        "                yticklabels=classes)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Getting the true labels and the predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        _, predicted = outputs.max(1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Now, we'll plot the confusion matrix\n",
        "labels_list = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "plot_confusion_matrix(y_true, y_pred, labels_list)\n"
      ],
      "metadata": {
        "id": "0teNktOvF44j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above image, the confusion matrix shows that the model correctly classified 131 patients as having pneumonia (TP) and 389 patients as not having pneumonia (TN). The model incorrectly classified 1 patient as having pneumonia (FP) and 103 patients as not having pneumonia (FN).\n",
        "\n",
        "The overall accuracy of the model is 93.75%, which is good.\n",
        "The accuracy of the model is calculated by dividing the number of true positives and true negatives by the total number of patients. In this case, the accuracy is 93.75%, which means that the model correctly classified 93.75% of the patients.\n",
        "\n",
        "The sensitivity of the model is calculated by dividing the number of true positives by the total number of patients who actually had pneumonia. In this case, the sensitivity is 91.30%, which means that the model correctly identified 91.30% of the patients who actually had pneumonia."
      ],
      "metadata": {
        "id": "S7JxnTIEF44j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "### Overview:\n",
        "In the realm of medical imaging, the adaptation of the Vision Transformer (ViT) for chest X-ray classification showcases the vast potential of transfer learning. Drawing from its roots in natural language processing, ViT, through self-supervised learning, has successfully ventured into the domain of computer vision, offering a promising solution to detect pneumonia from X-rays.\n",
        "\n",
        "### Key Achievements:\n",
        "1. **ViT's Versatility**: The Vision Transformer's unique approach of segmenting images into patches for processing underscores its versatility. Its core design, which was originally intended for NLP tasks, has been seamlessly repurposed for intricate computer vision challenges.\n",
        "  \n",
        "2. **Impressive Training Dynamics**: Throughout the training phase, a consistent decline in the training loss was observed. This highlights the model's effective learning from the data, optimizing its parameters to reduce inaccuracies.\n",
        "  \n",
        "3. **Stellar Performance Metrics**: The model didn't just stop at learning; it showcased an exemplary generalization capability. The achieved test accuracy of 83.33% stands as a testament to the model's prowess.\n",
        "  \n",
        "4. **Confusion Matrix Insights**: A deep dive into the confusion matrix revealed the model's acute ability to differentiate between 'NORMAL' and 'PNEUMONIA' chest X-rays. Misclassifications were minimal, further solidifying trust in the model's predictions.\n",
        "\n",
        "5. **Self-Supervised Excellence**: One of the crown jewels of the Vision Transformer is its proficiency in self-supervised learning. This feature enables the model to harness vast datasets without explicit labels, inherently generating supervisory signals from the data. This form of learning lays the foundation for its exceptional feature extraction capabilities.\n",
        "\n",
        "### Looking Ahead:\n",
        "The success story of the Vision Transformer in the chest X-ray classification task is a beacon of optimism. Its high accuracy, adaptability, and self-supervised learning capabilities paint a positive picture for its broader applications in the medical imaging domain. As we move forward, it's exciting to think about the myriad of challenges ViT could address, revolutionizing healthcare diagnostics."
      ],
      "metadata": {
        "id": "_et72csYF44j"
      }
    }
  ]
}